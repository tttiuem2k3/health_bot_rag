from typing import Union, List
from langchain_chroma import Chroma
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
import pickle
import torch

class VectorDB:
    def __init__(self,
                 documents: List[Document] = None,
                 vector_db: Union[Chroma, FAISS] = Chroma,
                 embedding_model: str = "bkai-foundation-models/vietnamese-bi-encoder",
                 load_db_path ="No"
                 ) -> None:
        
        self.vector_db = vector_db
        self.embedding = HuggingFaceEmbeddings(model_name=embedding_model)
        if load_db_path == "No":
            self.db = self._build_db(documents)
        else:
            self.db = self._load_embedded_data(load_db_path, embedding_model)

    def _preprocess_documents(self, documents: List[Document]) -> List[Document]:
        
        processed_documents = []
        for doc in documents:
            tokenized_content = " ".join(doc.page_content.split())  # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
            processed_doc = Document(page_content=tokenized_content, metadata=doc.metadata)
            processed_documents.append(processed_doc)
        return processed_documents

    def _build_db(self, documents: List[Document]):
        """
        XÃ¢y dá»±ng cÆ¡ sá»Ÿ dá»¯ liá»‡u vector tá»« danh sÃ¡ch tÃ i liá»‡u.
        :param documents: Danh sÃ¡ch cÃ¡c tÃ i liá»‡u.
        :return: CÆ¡ sá»Ÿ dá»¯ liá»‡u vector.
        """
        print(f"ğŸ§  Äang Embedding dá»¯ liá»‡u Data...")
        if documents:
            # Tiá»n xá»­ lÃ½ tÃ i liá»‡u trÆ°á»›c khi táº¡o embedding
            documents = self._preprocess_documents(documents)
        db = self.vector_db.from_documents(documents=documents, 
                                           embedding=self.embedding)
        print("âœ… Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c náº¡p vÃ o Chroma Database.")
        return db
    
    def _load_embedded_data(self, persist_directory: str, embedding_model: str):
        device = "cuda" if torch.cuda.is_available() else "cpu"
        embedding = HuggingFaceEmbeddings(model_name=embedding_model, model_kwargs={"device": device})
        vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)
        print("âœ… ÄÃ£ load dá»¯ liá»‡u embedding tá»« Chroma Database.")
        return vectordb
        
    
    def get_retriever(self, 
                      search_type: str = "similarity", 
                      search_kwargs: dict = {"k": 5}
                      ):
        """
        Táº¡o retriever Ä‘á»ƒ tÃ¬m kiáº¿m tÃ i liá»‡u dá»±a trÃªn Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng vector.
        :param search_type: Loáº¡i tÃ¬m kiáº¿m (máº·c Ä‘á»‹nh lÃ  "similarity").
        :param search_kwargs: Tham sá»‘ tÃ¬m kiáº¿m (vÃ­ dá»¥: sá»‘ lÆ°á»£ng káº¿t quáº£ "k").
        :return: Retriever.
        """
        retriever = self.db.as_retriever(search_type=search_type,
                                         search_kwargs=search_kwargs)
        return retriever