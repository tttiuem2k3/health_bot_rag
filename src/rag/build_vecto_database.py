import os
import pandas as pd
from typing import List
from tqdm import tqdm
import torch
from langchain.schema import Document
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from pyvi import ViTokenizer


# HÃ m tiá»n xá»­ lÃ½ vÄƒn báº£n tiáº¿ng Viá»‡t
def tokenize_text(text: str):
    return ViTokenizer.tokenize(text.strip())

# ========================
# 1. CSV Loader cÃ³ tiá»n xá»­ lÃ½
# ========================
class CSVLoader():
    def __init__(self) -> None:
        super().__init__()

    def __call__(self, csv_files: List[str], **kwargs):
        documents = []
        for csv_file in csv_files:
            data = pd.read_csv(csv_file)
            data = data.fillna("")

            for _, row in data.iterrows():
                # Tiá»n xá»­ lÃ½ vÄƒn báº£n
                processed_content = tokenize_text(row["Document"].strip())

                document = Document(
                    page_content=processed_content,
                    metadata={
                        "title": row["Title"].strip().lower(),
                        "source": row["Source"].strip(),
                    }
                )
                documents.append(document)
        print(f"ðŸ“„ Sá»‘ lÆ°á»£ng document: {len(documents)}")
        return documents
# ========================
# 2. Tiá»n xá»­ lÃ½ vÃ  chia Ä‘oáº¡n
# ========================
def preprocess_documents(csv_file: str, chunk_size: int = 1024, chunk_overlap: int = 200):
    loader = CSVLoader()
    documents = loader([csv_file])

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    doc_split = splitter.split_documents(documents)
    print(f"âœ‚ï¸ Sá»‘ lÆ°á»£ng chunk sau khi chia: {len(doc_split)}")
    # In ra sá»‘ lÆ°á»£ng tá»« trong 5 chunk Ä‘áº§u tiÃªn
    print("\nðŸ“Š Thá»‘ng kÃª sá»‘ lÆ°á»£ng tá»« trong má»™t sá»‘ chunk:")
    for i, doc in enumerate(doc_split[:5]):
        word_count = len(doc.page_content.split())
        print(f"Chunk {i+1}: {word_count} tá»« â€” Metadata: {doc.metadata}")
        print(f"Ná»™i dung: {doc.page_content[:100]}...\n")
    return doc_split

# ========================
# 3. Embedding + LÆ°u vÃ o Chroma
# ========================
def build_chroma_index(documents: List[Document], embedding_model: str, persist_directory: str):
    # Sá»­ dá»¥ng GPU náº¿u cÃ³
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ðŸ’» Thiáº¿t bá»‹ Ä‘ang dÃ¹ng: {device}")
    print(f"Sample document: {documents[:1]}")
    embedding = HuggingFaceEmbeddings(model_name=embedding_model, model_kwargs={"device": device})

    print("ðŸ” Äang embed vÃ  lÆ°u vÃ o Chroma...")
    vectordb = Chroma.from_documents(
        documents=tqdm(documents, desc="âš™ï¸ Äang xá»­ lÃ½ embedding"),
        embedding=embedding,
        persist_directory=persist_directory
    )
    vectordb.persist()
    print(f"âœ… ÄÃ£ lÆ°u index táº¡i: {persist_directory}")
    return vectordb

# ========================
# 4. Load láº¡i DB
# ========================
def load_chroma_index(embedding_model: str, persist_directory: str):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    embedding = HuggingFaceEmbeddings(model_name=embedding_model, model_kwargs={"device": device})
    vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)
    print("âœ… ÄÃ£ load láº¡i index tá»« disk.")
    return vectordb


# ========================
# 5. Main
# ========================
Data_health_path ="./data_source/health_bot"
csv_path = Data_health_path + "/disease_cleaned_final.csv"  # Thay báº±ng file CSV cá»§a báº¡n
persist_dir = Data_health_path + "/chroma_db1"
embedding_model = "bkai-foundation-models/vietnamese-bi-encoder"

doc_chunks = preprocess_documents(csv_path)
build_chroma_index(doc_chunks, embedding_model, persist_dir)


from langchain_community.vectorstores import Chroma
from pyvi import ViTokenizer

def preprocess_query(query: str) -> str:
    return ViTokenizer.tokenize(query.strip())

def load_disease_list(disease_file):
    with open(disease_file, "r", encoding="utf-8") as f:
        return [line.strip().lower() for line in f.readlines() if line.strip()]

def detect_disease_in_query(query: str, disease_list: list):
    query_lower = query.lower()
    sorted_disease_list = sorted(disease_list, key=len, reverse=True)
    for disease in sorted_disease_list:
        if disease in query_lower:
            return disease
    return None

def get_relevant_documents(query: str, vectordb: Chroma, disease_list_file, k: int = 10,detected_disease: str ="None" ):
    if detected_disease:
        print(f"ðŸ” PhÃ¡t hiá»‡n tÃªn bá»‡nh trong cÃ¢u há»i: {detected_disease}")
        retriever = vectordb.as_retriever(
            search_kwargs={"k": k, "filter": {"title": detected_disease}}
        )
    else:
        print("ðŸ”Ž KhÃ´ng phÃ¡t hiá»‡n tÃªn bá»‡nh, truy váº¥n toÃ n bá»™.")
        retriever = vectordb.as_retriever(search_kwargs={"k": k})

    return retriever.get_relevant_documents(query)

# Load Chroma Ä‘Ã£ build sáºµn
disease_list_file = Data_health_path +"/disease_list.txt"

vectordb = load_chroma_index(embedding_model, persist_dir)

# Truy váº¥n
query = "Gáº§n Ä‘Ã¢y tÃ´i hay Ä‘i tiá»ƒu Ä‘Ãªm"
disease_list = load_disease_list(disease_list_file)
detected_disease = detect_disease_in_query(query, disease_list)
k=10
query = preprocess_query(query)
docs = get_relevant_documents(query, vectordb,disease_list_file,k,detected_disease)
for doc in docs:
    print(doc.metadata["title"].capitalize(),"---",doc.metadata["source"],"\n--->",doc.page_content, "\n---------")
